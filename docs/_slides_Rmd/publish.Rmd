---

---

## Publishing your data

Choosing to publish your data products in a long-term repository can:

   - enable versioning of your data
   - fulfill a journal requirement to publish data
   - facilitate citing your dataset by assigning a permanent uniquely identifiable code (DOI)
   - improve discovery of and access to your dataset for others
   - enable re-use and greater visibility of your work

===

### When to publish your data?

Near beginning?  At very end?  

A couple issues to think about: 

1) How do you know you're using the same version of a file as your collaborator?

   Publish your data privately and use the unique ids of datasets to manage versions.

2) How do you control access to your data until you're ready to go public with it?

   Embargoing - controlling access for a certain period of time and then making your data public.

===

### Get an ORCiD 
 
ORCiDs identify you and link you to your publications and research products.  They are used by journals, repositories, etc. and often as a log in.  
![]({{ site.baseurl }}/images/Sign_In_snap.PNG){: width="100%"} 

To obtain an ORCiD, register at [https://orcid.org]: https://orcid.org.
![]({{ site.baseurl }}/images/orcid_snap.PNG){: width="100%"} 

===

### Picking a repository 

There are many repositories out there, and it can seem overwhelming picking a suitable one for your data. 

Repositories can be subject or domain specific.  [re3data]: https://www.re3data.org/ lists repositories by subject and can help you pick an appropriate repository for your data.  

===

[DataONE]: https://www.dataone.org/ is a federation of repositories housing many different types of data.  Some of these repositories include Knowledge Network for Biocomplexity[(KNB)]: https://knb.ecoinformatics.org/, Environmental Data Initiative [(EDI)]: https://environmentaldatainitiative.org/, [Dryad]: https://datadryad.org/, [USGS Science Data Catalog]: https://data.usgs.gov/datacatalog/

For qualitative data, there are a few dedicated repositories: [QDR]: https://qdr.syr.edu/, [Data-PASS]: http://data-pass.org/

===

Though a bit different, [Zenodo]: https://zenodo.org/ facilitates publishing (with a DOI) and archiving all research outputs from all research fields.  This can be used to publish releases of your code that lives in a GitHub repository.  However, since GitHub is not designed for data storage and is not a persistent repository, this is not a recommended way to store or publish data.  

===

### Uploading to a repository

Uploading requirements can vary by repository and type of data.  The minimum you usually need is basic metadata, and a data file.  

If you have a small number of files, using a repository GUI will usually be simpler.  For large numbers of files, automating uploads from R will save time.  

===

First we'll create a local data package using [`datapack`]: https://docs.ropensci.org/datapack/:

```{r}
library(datapack) ; library(uuid)

dp <- new("DataPackage") # create empty data package
```

Add the metadata file we created earlier to the blank data package.
```{r}
emlFile <- "data_package/metadata/dataspice.xml"
emlId <- paste("urn:uuid:", UUIDgenerate(), sep = "")

mdObj <- new("DataObject", id = emlId, format = "eml://ecoinformatics.org/eml-2.1.1", file = emlFile)

dp <- addMember(dp, mdObj)  # add metadata file to data package
```

===

Add the data file we saved earlier to the data package.
```{r}
datafile <- "data_package/StormEvents_d2006.csv"
dataId <- paste("urn:uuid:", UUIDgenerate(), sep = "")

dataObj <- new("DataObject", id = dataId, format = "text/csv", filename = datafile) 

dp <- addMember(dp, dataObj) # add data file to data package
```

Define the relationship between the data and metadata. 
```{r}
dp <- insertRelationship(dp, subjectID = emlId, objectIDs = dataId)
```

You can also add scripts and derived data files to the data package.  

===

Create a Resource Description Framework (RDF) of the relationships between data and metadata.
```{r}
serializationId <- paste("resourceMap", UUIDgenerate(), sep = "")
filePath <- file.path(sprintf("%s/%s.rdf", tempdir(), serializationId))
status <- serializePackage(dp, filePath, id=serializationId, resolveURI = "")
```

Save the data package to a file, using the [BagIt]: https://tools.ietf.org/id/draft-kunze-bagit-16.html packaging format.  
```{r}
dp_bagit <- serializeToBagIt(dp) 
```

===

Now to upload our data package to a repository in the DataONE federation, we need to use the `rdataone` package.  

1) Get authentication token for DataONE (follow steps [here]: https://github.com/DataONEorg/rdataone/blob/master/vignettes/dataone-federation.Rmd)

2) Upload your data package from R (from vignette for [rdataone]: https://github.com/DataONEorg/rdataone/blob/master/vignettes/upload-data.Rmd)

===

You can set the level of access to your data package.  
##################################################### To grant read permission to all users (?????? Is this just for a file or for the whole package???):
```{r}
sourceObj <- setPublicAccess(sourceObj)
```

<!-- To make your data package private:  -->
<!-- ```{r} -->
<!-- sourceObj <- setPrivateAccess(sourceObj) -->
<!-- ``` -->


===

### Citation

Getting a DOI








